{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IU Sketch Baseline 코드  \n",
    " - python모듈 설치 코드는 처음 한번 실행해주세요.     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install imageio\n",
    "# !pip install imageio --upgrade\n",
    "# !pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 path 설정.\n",
    "import os,sys\n",
    "sys.path.insert(1, os.path.join(os.getcwd()  , '..'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "베이스 라인 코드.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-02 14:14:48.046498: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-02 14:14:48.265708: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-02-02 14:14:49.214919: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-02 14:14:49.215031: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-02 14:14:49.215044: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17187836877807923279\n",
      "xla_global_id: -1\n",
      "]\n",
      "WARNING:tensorflow:From /tmp/ipykernel_7201/111514551.py:38: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-02 14:14:50.974055: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-02 14:14:51.006524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-02 14:14:51.006923: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/evergrin/anaconda3/envs/pyenv_3912/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-02-02 14:14:51.007000: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/evergrin/anaconda3/envs/pyenv_3912/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-02-02 14:14:51.007067: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/evergrin/anaconda3/envs/pyenv_3912/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-02-02 14:14:51.007130: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/evergrin/anaconda3/envs/pyenv_3912/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-02-02 14:14:51.007195: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/evergrin/anaconda3/envs/pyenv_3912/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-02-02 14:14:51.010146: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: 동적 오브젝트 파일을 열 수 없습니다: 그런 파일이나 디렉터리가 없습니다; LD_LIBRARY_PATH: /home/evergrin/anaconda3/envs/pyenv_3912/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-02-02 14:14:51.010166: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-02-02 14:14:51.063365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-02 14:14:51.063683: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import os, glob, random\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import ThresholdedReLU\n",
    "\n",
    "from tensorflow.keras import layers, Input, Model\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "import apps.config_env as cfg\n",
    "\n",
    "from classes.image_frame import ImgFrame\n",
    "from classes.video_clip import VideoClip\n",
    "from models.dataset_generator import DataSetGenerator\n",
    "from models.draw_generator import DrawGenerator\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# from models.layer_conv import Conv2Plus1D, TConv2Plus1D\n",
    "# from models.layer_encoder import Encoder5D, Decoder5D\n",
    "# from models.layer_lstm import ConvLstmSeries\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(tf.__version__)\n",
    "print(device_lib.list_local_devices())\n",
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 디렉토리 없으면 생성.\n",
    "\n",
    "# 학습용 raw_clip(gif) 파일 위치.\n",
    "if not os.path.exists(cfg.RAW_CLIP_PATH):\n",
    "    os.mkdir(cfg.RAW_CLIP_PATH)\n",
    "\n",
    "# 모델 저장 위치\n",
    "if not os.path.exists(cfg.MODEL_SAVE_PATH):\n",
    "    os.mkdir(cfg.MODEL_SAVE_PATH)\n",
    "\n",
    "# 임시 데이터 저장 위치\n",
    "if not os.path.exists(cfg.TEMP_DATA_PATH):\n",
    "    os.mkdir(cfg.TEMP_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 15    # matlab 차트의 기본 크기를 15,6으로 지정해 줍니다.\n",
    "\n",
    "def plot_history(history):\n",
    "    \n",
    "    # summarize history for loss  \n",
    "    plt.subplot(211)  \n",
    "    plt.plot(history['tgen_loss'])  \n",
    "    plt.plot(history['tdisc_loss'])  \n",
    "    plt.plot(history['tl1_loss'])  \n",
    "    plt.plot(history['vgen_loss'])  \n",
    "    plt.plot(history['vdisc_loss'])  \n",
    "    plt.plot(history['vl1_loss'])  \n",
    "    plt.title('model loss')  \n",
    "    plt.ylabel('loss')  \n",
    "    plt.xlabel('batch iters')  \n",
    "    plt.legend(['tgen_loss', 'tdisc_loss', 'tl1_loss', 'vgen_loss', 'vdisc_loss', 'vl1_loss'], loc='upper right')  \n",
    "\n",
    "    # summarize history for accuracy  \n",
    "    # plt.subplot(212)  \n",
    "    # plt.plot(history['fake_accuracy'])  \n",
    "    # plt.plot(history['real_accuracy'])  \n",
    "    # plt.title('discriminator accuracy')  \n",
    "    # plt.ylabel('accuracy')  \n",
    "    # plt.xlabel('batch iters')  \n",
    "    # plt.legend(['fake_accuracy', 'real_accuracy'], loc='upper right')  \n",
    "    \n",
    "    # training_history 디렉토리에 epoch별로 그래프를 이미지 파일로 저장합니다.\n",
    "    # plt.savefig(os.path.join(history_path, 'train_history_{:04d}.png'.format(epoch)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arry5d_to_img(arry5d, save_as='', threshold=0.0):\n",
    "    frmimg_cnt = arry5d.shape[1]\n",
    "    fig, axes = plt.subplots(nrows = 1, ncols = frmimg_cnt, figsize=(15, 3))\n",
    "\n",
    "    for idx, num in enumerate(range(0, frmimg_cnt)):\n",
    "        frm = ImgFrame(img=arry5d[0][idx][:, :, :], do_norm=False)\n",
    "        min_val = np.min(frm.arry)\n",
    "        max_val = np.max(frm.arry)\n",
    "        frm.arry = (frm.arry - min_val) / (max_val - min_val)\n",
    "\n",
    "        min_val = np.min(frm.arry)\n",
    "        max_val = np.max(frm.arry)\n",
    "        # print(\"min,max: \", np.min(frm.arry), np.max(frm.arry))\n",
    "\n",
    "        if threshold > 0.0:\n",
    "            frm.threshold(threshold=threshold)\n",
    "\n",
    "        img = frm.to_flatten_image()\n",
    "        axes[idx].imshow(img, cmap='gray')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_imgs(arry1, arry2, save_path=\"\"):\n",
    "    frmimg_cnt = arry1.shape[1]\n",
    "    fig, axes = plt.subplots(nrows = 2, ncols = frmimg_cnt, figsize=(15, 3))\n",
    "    arrys = [arry1, arry2]\n",
    "    \n",
    "    for i, arry in enumerate(arrys):\n",
    "        for idx, num in enumerate(range(0, frmimg_cnt)):\n",
    "            frm = ImgFrame(img=arry[0][idx][:, :, :], do_norm=False)\n",
    "            min_val = np.min(frm.arry)\n",
    "            max_val = np.max(frm.arry)\n",
    "            frm.arry = (frm.arry - min_val) / (max_val - min_val)\n",
    "\n",
    "            min_val = np.min(frm.arry)\n",
    "            max_val = np.max(frm.arry)\n",
    "            # print(\"min,max: \", np.min(frm.arry), np.max(frm.arry))\n",
    "\n",
    "            img = frm.to_image()\n",
    "            axes[i][idx].imshow(img, cmap='gray')\n",
    "\n",
    "    if save_path != \"\":\n",
    "        plt.savefig(save_path)\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_w, img_h = 64, 64 #cfg.DATA_IMG_W, cfg.DATA_IMG_H\n",
    "batch_size = 5 #cfg.DATA_BATCH_SIZE\n",
    "time_steps = 5 # cfg.DATA_TIME_STEP\n",
    "enc_blk_count = 5  # 3 - 7\n",
    "disc_blk_count = 3 # \n",
    "EPOCHS = 100\n",
    "\n",
    "enc_filters = [64,128,256,512,512,512,512,512]\n",
    "dec_filters = [512,512,512,512,256,128,64]\n",
    "use_conv2n1 = False\n",
    "train_lamda = 100\n",
    "\n",
    "# discriminator augmentation\n",
    "use_ada = False\n",
    "ada_p = 0.7\n",
    "ada_lamda1 = 10\n",
    "ada_lamda2 = 10\n",
    "\n",
    "# dataset 설정.\n",
    "data_seq_type = 'aforward'  # 'all', 'rest', 'arandom', 'aforward', 'forward', 'reverse', 'random'\n",
    "data_label_type = '1step'   # 'all', 'rest', 'same', '1step'\n",
    "stakced = True\n",
    "overlap = False\n",
    "fill_box = False\n",
    "invert = False\n",
    "\n",
    "save_load_prefix = \"test_00\"\n",
    "save_load_preload = False\n",
    "save_load_interval = 0\n",
    "\n",
    "# 전체 raw_clip 랜덤한 이미지 목록을 가져옴.\n",
    "img_list = glob.glob(os.path.join(cfg.RAW_CLIP_PATH, \"*.gif\"))\n",
    "random.shuffle(img_list)\n",
    "\n",
    "# 이미지 목록을 train/validation용으로 9:1로 나눔.\n",
    "train_val_ratio = 0.9\n",
    "train_img_cnt = int(len(img_list) * train_val_ratio)\n",
    "train_img_list = img_list[:train_img_cnt]\n",
    "val_img_list = img_list[train_img_cnt:]\n",
    "\n",
    "# train/validation용 generator를 생성.\n",
    "# tdgen = DataSetGenerator(imgs=train_img_list, batch_size=batch_size, \n",
    "#                          time_step=time_steps, imgw=img_w, imgh=img_h, \n",
    "#                          seq_type=data_seq_type, label_type=data_label_type,\n",
    "#                          stacked=stakced, overlap=overlap, fill_box=fill_box, invert=invert)\n",
    "\n",
    "# vdgen = DataSetGenerator(imgs=val_img_list, batch_size=batch_size, \n",
    "#                          time_step=time_steps, imgw=img_w, imgh=img_h, \n",
    "#                          seq_type=data_seq_type, label_type=data_label_type,\n",
    "#                          stacked=stakced, overlap=overlap, fill_box=fill_box, invert=invert)\n",
    "\n",
    "tdgen = DrawGenerator(imgs=train_img_list, batch_size=batch_size, \n",
    "                         time_step=time_steps, imgw=img_w, imgh=img_h)\n",
    "\n",
    "vdgen = DrawGenerator(imgs=val_img_list, batch_size=batch_size, \n",
    "                         time_step=time_steps, imgw=img_w, imgh=img_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show sample data\n",
    "it = iter(tdgen)\n",
    "x, y = next(it)\n",
    "\n",
    "print(f\"train data : {len(train_img_list)}, val data : {len(val_img_list)}\")\n",
    "print(f\"x type: {x.shape}, y type: {y.shape}\")\n",
    "\n",
    "arry5d_to_img(x, threshold=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "arry5d_to_img(y, threshold=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodeBlock(layers.Layer):\n",
    "    def __init__(self, n_filters, use_bn=True):\n",
    "        super(EncodeBlock, self).__init__()\n",
    "        self.use_bn = use_bn       \n",
    "        self.conv = layers.Conv3D(filters=n_filters,\n",
    "                    kernel_size=(1, 4, 4),\n",
    "                    strides=(1, 2, 2),\n",
    "                    padding=\"same\")\n",
    "        self.conv2 = layers.Conv3D(filters=n_filters,\n",
    "                    kernel_size=(4, 1, 1),\n",
    "                    strides=1,\n",
    "                    padding=\"same\")        \n",
    "        self.batchnorm = layers.BatchNormalization()\n",
    "        self.lrelu= layers.LeakyReLU(0.2)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        x = self.conv(x, training=training)\n",
    "        if use_conv2n1:\n",
    "            x = self.conv2(x, training=training)\n",
    "        if self.use_bn:\n",
    "            x = self.batchnorm(x, training=training)\n",
    "#         x = layers.Dropout(.5)(x, training=training)\n",
    "        return self.lrelu(x)\n",
    "\n",
    "\n",
    "class Encoder(Model):\n",
    "    def __init__(self, blk_cnt=5):\n",
    "        super(Encoder, self).__init__()\n",
    "#         filters = [64,128,256,512,512,512,512,512]\n",
    "        \n",
    "        self.blocks = []\n",
    "        for i in range(blk_cnt):\n",
    "            f = enc_filters[i]\n",
    "            if i == 0:\n",
    "                self.blocks.append(EncodeBlock(f, use_bn=False))\n",
    "            else:\n",
    "                self.blocks.append(EncodeBlock(f))\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        for block in self.blocks:\n",
    "            x = block(x, training=training)\n",
    "        return x\n",
    "    \n",
    "    def get_summary(self, input_shape=(None, img_w, img_h, 1)):\n",
    "        inputs = Input(input_shape)\n",
    "        return Model(inputs, self.call(inputs)).summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecodeBlock(layers.Layer):\n",
    "    def __init__(self, f, dropout=True):\n",
    "        super(DecodeBlock, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.Transconv = layers.Conv3DTranspose(filters=f,\n",
    "                    kernel_size=(1, 4, 4),\n",
    "                    strides=(1, 2, 2),\n",
    "                    padding=\"same\")\n",
    "        self.Transconv2 = layers.Conv3DTranspose(filters=f, \n",
    "                    kernel_size=(4, 1, 1),\n",
    "                    strides=1,\n",
    "                    padding=\"same\")        \n",
    "        self.batchnorm = layers.BatchNormalization()\n",
    "        self.relu = layers.ReLU()\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        x = self.Transconv(x, training=training)\n",
    "        if use_conv2n1:\n",
    "            x = self.Transconv2(x, training=training)\n",
    "        x = self.batchnorm(x, training=training)\n",
    "        if self.dropout:\n",
    "            x = layers.Dropout(.5)(x, training=training)\n",
    "        return self.relu(x)\n",
    "\n",
    "    \n",
    "class Decoder(Model):\n",
    "    def __init__(self, blk_cnt=4):\n",
    "        super(Decoder, self).__init__()\n",
    "#         filters = [512,512,512,512,256,128,64]\n",
    "        \n",
    "        self.blocks = []\n",
    "        for i in range(blk_cnt):\n",
    "            f = dec_filters[i - blk_cnt]\n",
    "            if i < 3:\n",
    "                self.blocks.append(DecodeBlock(f))\n",
    "            else:\n",
    "                self.blocks.append(DecodeBlock(f, dropout=False))\n",
    "                \n",
    "        self.blocks.append(layers.Conv3DTranspose(filters=1,\n",
    "                    kernel_size=(1, 4, 4),\n",
    "                    strides=(1, 2, 2),\n",
    "                    padding=\"same\"))\n",
    "        if use_conv2n1:\n",
    "            self.blocks.append(layers.Conv3DTranspose(filters=1,\n",
    "                    kernel_size=(4, 1, 1),\n",
    "                    strides=(1, 1, 1),\n",
    "                    padding=\"same\"))\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLstmSeries(Model):\n",
    "    '''\n",
    "        5-rank Colvolution LSTM\n",
    "    '''\n",
    "    def __init__(self, filter_cnt, final_filter_cnt, kernel_sizes):\n",
    "        \"\"\"\n",
    "            (b, t, h, w, c) -> (b, t, h/stride, w/stride, f)\n",
    "            A sequence of convolutional layers that first apply the convolution operation over the\n",
    "            spatial dimensions, and then the temporal dimension.\n",
    "        \"\"\"\n",
    "        super(ConvLstmSeries, self).__init__()\n",
    "        self.filter_cnt = filter_cnt\n",
    "        self.final_filter_cnt = final_filter_cnt\n",
    "        self.kernel_cnt = len(kernel_sizes)\n",
    "        self.lstms = []\n",
    "        self.bns = []\n",
    "        self.relus = []\n",
    "        self.out_conv = None\n",
    "\n",
    "        for kernel_size in kernel_sizes:\n",
    "            self.lstms.append(layers.ConvLSTM2D(\n",
    "                                filters=filter_cnt,\n",
    "                                kernel_size=kernel_size,\n",
    "                                padding=\"same\",\n",
    "                                return_sequences=True,\n",
    "                                #recurrent_activation='hard_sigmoid', # other nan... layers.ReLU(), #layers.LeakyReLU(0.2), #'hard_sigmoid',\n",
    "                                activation=layers.ReLU(), #layers.LeakyReLU(0.2), #\"relu\",\n",
    "                                kernel_regularizer=\"l2\",\n",
    "                                recurrent_regularizer=\"l2\",\n",
    "                                activity_regularizer=\"l2\",\n",
    "                            ))\n",
    "            self.bns.append(layers.BatchNormalization())\n",
    "            self.relus.append(layers.LeakyReLU(0.2)) #layers.ReLU()\n",
    "\n",
    "        # 출력의 channel depth를 맞춰주기 위해.\n",
    "        if final_filter_cnt > 0:\n",
    "            self.out_conv = layers.Conv3D(filters=final_filter_cnt,\n",
    "                    kernel_size=(1, 3, 3),\n",
    "                    strides=(1, 1, 1),\n",
    "                    padding=\"same\")\n",
    "            self.out_bn = layers.BatchNormalization()\n",
    "            self.out_relu = keras.activations.sigmoid # layers.LeakyReLU(0.2) # keras.activations.sigmoid # layers.ReLU()\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        for idx in range(self.kernel_cnt):\n",
    "            x = self.lstms[idx](x, training=training)\n",
    "            x = self.bns[idx](x, training=training)\n",
    "            x = layers.Dropout(.5)(x, training=training)\n",
    "            x = self.relus[idx](x)\n",
    "\n",
    "        if self.out_conv is not None:\n",
    "            x = self.out_conv(x, training=training)\n",
    "            x = self.out_bn(x, training=training)\n",
    "            x = layers.Dropout(.5)(x, training=training)\n",
    "            x = self.out_relu(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoderGenerator(Model):\n",
    "    def __init__(self, blk_cnt=4):\n",
    "        super(EncoderDecoderGenerator, self).__init__()\n",
    "        self.encoder = Encoder(blk_cnt)\n",
    "        self.decoder = Decoder(blk_cnt-1)\n",
    "        \n",
    "        lstm_filters = enc_filters[blk_cnt-1]\n",
    "        self.lstms = ConvLstmSeries(lstm_filters, 0, [(5, 5), (3, 3), (1, 1)])\n",
    "#         self.lstms = ConvLstmSeries(lstm_filters, 1, [(3, 3), (3, 3), (3, 3)])\n",
    "    \n",
    "    ''' \n",
    "        아래와 같이 순서와 조합을 바꾸면 다른 모델이 됨.\n",
    "        이때 loss 및 save/load도 같이 바뀌므로 주의.\n",
    "            1. encoder - decoder(pix2pix gan model)\n",
    "            2. lstms\n",
    "            3. encoder(pretrained) - lstms - decoder(pretrained)\n",
    "            4. lstms(pre-trained) - encoder - decoder\n",
    "    '''\n",
    "    def call(self, x, training=False):\n",
    "        x = self.encoder(x, training=training)\n",
    "        # x = self.lstms(x, training=training)\n",
    "        x = self.decoder(x, training=training)\n",
    "        return x\n",
    "   \n",
    "    def get_summary(self, input_shape=(None, img_w, img_h, 1)):\n",
    "        inputs = Input(input_shape)\n",
    "        return Model(inputs, self.call(inputs)).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscBlock(layers.Layer):\n",
    "    def __init__(self, n_filters, stride=2, custom_pad=0, use_bn=True, act=True):\n",
    "        super(DiscBlock, self).__init__()\n",
    "        self.custom_pad = custom_pad\n",
    "        self.use_bn = use_bn\n",
    "        self.act = act\n",
    "        \n",
    "        # outputsize = (w - f + 2*p) / s + 1\n",
    "        if custom_pad > 0:\n",
    "            self.padding = layers.ZeroPadding3D(padding=(0,custom_pad,custom_pad))\n",
    "            self.conv = layers.Conv3D(filters=n_filters,\n",
    "                    kernel_size=(1, 4, 4),\n",
    "                    strides=(1, stride, stride),\n",
    "                    padding=\"valid\")\n",
    "            self.conv2 = layers.Conv3D(filters=n_filters,\n",
    "                    kernel_size=(4, 1, 1),\n",
    "                    strides=(1, 1, 1),\n",
    "                    padding=\"valid\")\n",
    "        else:\n",
    "            self.conv = layers.Conv3D(filters=n_filters,\n",
    "                    kernel_size=(1, 4, 4),\n",
    "                    strides=(1, stride, stride),\n",
    "                    padding=\"same\")\n",
    "            self.conv2 = layers.Conv3D(filters=n_filters,\n",
    "                    kernel_size=(4, 1, 1),\n",
    "                    strides=(1, 1, 1),\n",
    "                    padding=\"same\")\n",
    "        \n",
    "        self.batchnorm = layers.BatchNormalization() if use_bn else None\n",
    "        self.lrelu = layers.LeakyReLU(0.2) if act else None\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        if self.custom_pad:\n",
    "            x = self.padding(x)\n",
    "            x = self.conv(x, training=training)\n",
    "            if use_conv2n1:\n",
    "                x = self.conv2(x, training=training)\n",
    "        else:\n",
    "            x = self.conv(x, training=training)\n",
    "            if use_conv2n1:\n",
    "                x = self.conv2(x, training=training)\n",
    "            \n",
    "        if self.use_bn:\n",
    "            x = self.batchnorm(x, training=training)\n",
    "\n",
    "        if training:\n",
    "            x = layers.Dropout(.5)(x, training=training)\n",
    "            \n",
    "        if self.act:\n",
    "            x = self.lrelu(x)\n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(Model):\n",
    "    def __init__(self, blk_cnt=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.concat = layers.Concatenate()\n",
    "\n",
    "        filters = [64,128,256,512,512,512]\n",
    "        self.blocks = []\n",
    "        for i in range(blk_cnt):\n",
    "            f = filters[i]\n",
    "            self.blocks.append(DiscBlock(\n",
    "                n_filters=f,\n",
    "                stride=2,\n",
    "                custom_pad=0,\n",
    "                use_bn=False if i==0 else True,\n",
    "                act=True\n",
    "            ))\n",
    "\n",
    "        self.blocks.append(DiscBlock(n_filters=512, stride=1, custom_pad=1, use_bn=True, act=True))\n",
    "        self.blocks.append(DiscBlock(n_filters=1, stride=1, custom_pad=1, use_bn=False, act=False))\n",
    "        self.sigmoid = layers.Activation(\"sigmoid\")\n",
    "\n",
    "\n",
    "    def call(self, x, y, training=False):\n",
    "        out = self.concat([x, y])\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            out = block(out, training=training)\n",
    "            \n",
    "        # if training:\n",
    "        #     out = layers.Dropout(.5)(out)\n",
    "                \n",
    "        return self.sigmoid(out)\n",
    "    \n",
    "    def get_summary(self, x_shape=(None, img_w, img_h, 1), y_shape=(None, img_w, img_h, 1)):\n",
    "        x, y = Input(x_shape), Input(y_shape) \n",
    "        return Model((x, y), self.call(x, y)).summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    '''\n",
    "    모델학습 초기에 learning rate를 급격히 높였다가, \n",
    "    서서히 낮추어 가면서 안정적으로 수렴하게 하는 고급 기법\n",
    "    학습 초기에는 learning_rate가 step_num에 비례해서 증가하다가 이후로는 감소\n",
    "    '''\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ADA(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(ADA, self).__init__()\n",
    "        self.flip = tf.image.flip_left_right\n",
    "        self.rot90 = tf.image.rot90\n",
    "        self.resize = tf.image.resize  # (sie, method, preserve_aspect_ratio)\n",
    "        self.cropnresize = tf.image.crop_and_resize #(image, boxes,box_indices,crop_size,method='bilinear',extrapolation_value=0.0,\n",
    "\n",
    "    \n",
    "    def call(self, x):\n",
    "        x_shape = x.shape\n",
    "        x_batch = x_shape[0]*x_shape[1]\n",
    "        x_h, x_w = (x_shape[2], x_shape[3])\n",
    "        x_channel = x_shape[4]\n",
    "\n",
    "        # 5d tensor -> 4d tensor\n",
    "        x1 = tf.reshape(x, [x_batch, x_h, x_w, x_channel])\n",
    "\n",
    "        x1 = self.flip(x1)\n",
    "        x1 = self.rot90(x1)\n",
    "        x1 = self.resize(x1, [int(x_h*1.5), int(x_w*1.5)])\n",
    "        x1 = self.resize(x1, [x_h, x_w])\n",
    "        \n",
    "        # 4d tensor -> 5d tensor\n",
    "        x2 = tf.reshape(x1, x_shape)\n",
    "        return x2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce = losses.BinaryCrossentropy(from_logits=True) # False\n",
    "mae = losses.MeanAbsoluteError()\n",
    "mse = tf.keras.metrics.MeanSquaredError()\n",
    "poisson = tf.keras.losses.Poisson()\n",
    "ada_loss = mse\n",
    "\n",
    "def dice_score_loss(y_true, y_pred):\n",
    "    numerator = 2. * tf.reduce_sum(y_true * y_pred)\n",
    "    denominator = tf.reduce_sum(y_true + y_pred)\n",
    "    return tf.reduce_mean(1 - numerator / denominator)\n",
    "\n",
    "def get_gene_loss(fake_output, real_output, fake_disc):\n",
    "    l1_loss = mae(real_output, fake_output)\n",
    "    gene_loss = bce(tf.ones_like(fake_disc), fake_disc)\n",
    "    return gene_loss, l1_loss\n",
    "\n",
    "def get_disc_loss(fake_disc, real_disc):\n",
    "    return bce(tf.zeros_like(fake_disc), fake_disc) + bce(tf.ones_like(real_disc), real_disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene_learning_rate = CustomSchedule(d_model=64, warmup_steps=5000)\n",
    "# disc_learning_rate = CustomSchedule(d_model=64, warmup_steps=5000)\n",
    "# gene_opt = optimizers.Adam(gene_learning_rate, beta_1=.5, beta_2=.999)\n",
    "# disc_opt = optimizers.Adam(disc_learning_rate, beta_1=.5, beta_2=.999)\n",
    "\n",
    "gene_opt = optimizers.Adam(2e-4, beta_1=.5, beta_2=.999)\n",
    "disc_opt = optimizers.Adam(2e-4, beta_1=.5, beta_2=.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = EncoderDecoderGenerator(blk_cnt=enc_blk_count)\n",
    "discriminator = Discriminator(blk_cnt=disc_blk_count)\n",
    "ada = ADA()\n",
    "\n",
    "generator.get_summary()\n",
    "discriminator.get_summary()\n",
    "\n",
    "# plot_model(discriminator, show_shapes=True, show_layer_names=True, expand_nested=True)\n",
    "\n",
    "history = {'tgen_loss':[], 'tdisc_loss':[], 'tl1_loss':[], 'vgen_loss':[], 'vdisc_loss':[], 'vl1_loss':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' gan model(train step)을 사용하지 않고 학습할때  '''\n",
    "\n",
    "# generator.compile(\n",
    "#     loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(),\n",
    "# )\n",
    "\n",
    "# early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10)\n",
    "# reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5)\n",
    "\n",
    "# generator.fit(\n",
    "#     tdgen,\n",
    "#     batch_size=batch_size,\n",
    "#     epochs=EPOCHS,\n",
    "#     validation_data=vdgen,\n",
    "#     callbacks=[early_stopping, reduce_lr],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all(prefix='lstm_only_500'):\n",
    "    save_path = os.path.join(cfg.MODEL_SAVE_PATH, prefix)\n",
    "#     generator.lstms.load_weights(os.path.join(save_path, f\"lstm_{prefix}\"))\n",
    "    generator.encoder.load_weights(os.path.join(save_path, f\"encoder_{prefix}\"))\n",
    "    generator.decoder.load_weights(os.path.join(save_path, f\"decoder_{prefix}\"))\n",
    "\n",
    "    discriminator.load_weights(os.path.join(save_path, f\"disc_{prefix}\"))\n",
    "    \n",
    "    hist_file = os.path.join(save_path, f\"history_{prefix}.pic\")\n",
    "\n",
    "    with open(file=hist_file, mode='rb') as f:\n",
    "        loaded_history = pickle.load(f)\n",
    "    \n",
    "    print(f'{prefix} loaded...')\n",
    "    return loaded_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all(prefix='base_200'):\n",
    "    save_path = os.path.join(cfg.MODEL_SAVE_PATH, prefix)\n",
    "    if not os.path.exists(save_path):\n",
    "        os.mkdir(save_path)\n",
    "    \n",
    "#     generator.lstms.save_weights(os.path.join(save_path, f\"lstm_{prefix}\"))\n",
    "    generator.encoder.save_weights(os.path.join(save_path, f\"encoder_{prefix}\"))\n",
    "    generator.decoder.save_weights(os.path.join(save_path, f\"decoder_{prefix}\"))\n",
    "\n",
    "    discriminator.save_weights(os.path.join(save_path, f\"disc_{prefix}\"))\n",
    "\n",
    "    hist_file = os.path.join(save_path, f\"history_{prefix}.pic\")\n",
    "    with open(file=hist_file, mode='wb') as f:\n",
    "        pickle.dump(history, f)\n",
    "        \n",
    "    print(f'{prefix} saved...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_load_preload:\n",
    "    try:\n",
    "        history = load_all(prefix=save_load_prefix)\n",
    "    except:\n",
    "        print('preload weight failed !!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def train_step(sketch, label, idx):\n",
    "    with tf.GradientTape() as gene_tape, tf.GradientTape() as disc_tape:\n",
    "        # Generator 예측\n",
    "        fake_label = generator(sketch, training=True)\n",
    "        \n",
    "        # Discriminator 예측\n",
    "        fake_disc = discriminator(sketch, fake_label, training=True)\n",
    "        real_disc = discriminator(sketch, label, training=True)\n",
    "\n",
    "        # Generator 손실 계산\n",
    "        gene_loss, l1_loss = get_gene_loss(fake_label, label, fake_disc)\n",
    "        gene_total_loss = gene_loss + (train_lamda * l1_loss) ## <===== L1 손실 반영 λ=100\n",
    "\n",
    "        # Discrminator 손실 계산\n",
    "        disc_loss = get_disc_loss(fake_disc, real_disc)\n",
    "        \n",
    "        if use_ada and random.random() < ada_p:\n",
    "            # Discriminator aug 예측\n",
    "            fake_aug = ada(fake_label)\n",
    "            sketch_aug = ada(sketch)\n",
    "            label_aug = ada(label)\n",
    "\n",
    "            fake_aug_disc = discriminator(sketch_aug, fake_aug, training=False)\n",
    "            real_aug_disc = discriminator(sketch_aug, label_aug, training=False)\n",
    "\n",
    "            fake_aug_loss = ada_loss(fake_aug_disc, fake_disc)\n",
    "            real_aug_loss = ada_loss(real_aug_disc, real_disc)\n",
    "            \n",
    "            disc_total_loss = disc_loss + fake_aug_loss*ada_lamda1 + real_aug_loss*ada_lamda2\n",
    "\n",
    "            #tf.print(disc_loss, fake_aug_loss, real_aug_loss, disc_total_loss)\n",
    "        else:\n",
    "            disc_total_loss = disc_loss\n",
    "                \n",
    "    gene_gradient = gene_tape.gradient(gene_total_loss, generator.trainable_variables)\n",
    "    disc_gradient = disc_tape.gradient(disc_total_loss, discriminator.trainable_variables)\n",
    "\n",
    "    gene_opt.apply_gradients(zip(gene_gradient, generator.trainable_variables))\n",
    "    disc_opt.apply_gradients(zip(disc_gradient, discriminator.trainable_variables))\n",
    "    # disc_opt.apply_gradients(zip(disc_gradient, discriminator.trainable_variables))\n",
    "    return gene_loss, l1_loss, disc_total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_step(sketch, label):\n",
    "    # Generator 예측\n",
    "    fake_label = generator(sketch, training=False)\n",
    "\n",
    "    # Discriminator 예측\n",
    "    fake_disc = discriminator(sketch, fake_label, training=False)\n",
    "    real_disc = discriminator(sketch, label, training=False)\n",
    "\n",
    "    # Generator 손실 계산\n",
    "    gene_loss, l1_loss = get_gene_loss(fake_label, label, fake_disc)\n",
    "    gene_total_loss = gene_loss + (100 * l1_loss) ## <===== L1 손실 반영 λ=100\n",
    "\n",
    "    # Discrminator 손실 계산\n",
    "    disc_loss = get_disc_loss(fake_disc, real_disc)\n",
    "\n",
    "    return gene_loss, l1_loss, disc_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(vdgen)\n",
    "\n",
    "def show_predicts(cnt=3, save_path=\"\"):\n",
    "    # dataset중 첫번째만 뽑아서 예측에 입력\n",
    "    try:\n",
    "        x, y = next(it)\n",
    "    except Exception as e:\n",
    "        it = iter(vdgen)\n",
    "        x, y = next(it)\n",
    "\n",
    "    # 연속 예측한 이미지 3개 표시.\n",
    "    frame_cnt = x.shape[1]\n",
    "    in_x = x[:1, :-cnt+1, :, :, :]\n",
    "    in_y = y[:1, :, :, :, :]\n",
    "\n",
    "    for i in range(cnt):\n",
    "        pred = generator(in_x)\n",
    "        in_x = np.append(in_x, pred[:, -1:, :, :, :], axis=1)\n",
    "\n",
    "    show_imgs(y[:, -5:], in_x[:, -5:], save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"================================= config =================================\")\n",
    "print(f\"img size : {img_w}x{img_h}\")\n",
    "print(f\"data : ({data_seq_type}, {data_label_type}, {time_steps}frames), stacked:{stakced}, overlap:{overlap}, fill_box : {fill_box}\")\n",
    "print(f\"enc_blk : {enc_blk_count}, disc_blk : {disc_blk_count}, ada : {use_ada} \")\n",
    "print(f\"use_conv2n1 : {use_conv2n1}, disc_blk : {disc_blk_count}, ada : {use_ada} \")\n",
    "print(\"==========================================================================\\n\")\n",
    "\n",
    "train_lamda = 100\n",
    "ada_lamda1 = 10\n",
    "ada_lamda2 = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPOCHS = 1000\n",
    "\n",
    "tit = iter(tdgen)\n",
    "vit = iter(vdgen)\n",
    "\n",
    "train_step_cnt = 2\n",
    "val_step_cnt = 1\n",
    "\n",
    "tg_loss, tl1_loss, td_loss = 0., 0., 0. \n",
    "vg_loss, vl1_loss, vd_loss = 0., 0., 0. \n",
    "\n",
    "print(f\"train started (batch: {batch_size}, epoch: {EPOCHS}, tstep:{train_step_cnt}, vstep:{val_step_cnt})\")\n",
    "\n",
    "batch_start = time()\n",
    "img_idx = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    start = time()\n",
    "\n",
    "    # train loop\n",
    "    g_losses = []\n",
    "    d_losses = []\n",
    "    l1_losses = []\n",
    "    for loop in range(train_step_cnt):\n",
    "\n",
    "        print(f\"\\rT {epoch + 1:4d} {loop+1:3d}\", end=\"\")\n",
    "\n",
    "        try:\n",
    "            x, y = next(tit)\n",
    "        except Exception as e:\n",
    "            tit = iter(tdgen)\n",
    "            x, y = next(tit)\n",
    "\n",
    "        tg_loss, tl1_loss, td_loss = train_step(x, y, loop)\n",
    "\n",
    "        g_losses.append(tg_loss.numpy())\n",
    "        d_losses.append(td_loss.numpy())\n",
    "        l1_losses.append(tl1_loss.numpy())\n",
    "\n",
    "    tg_loss = np.average(np.array(g_losses))\n",
    "    td_loss = np.average(np.array(d_losses))\n",
    "    tl1_loss = np.average(np.array(l1_losses))\n",
    "    \n",
    "    history['tgen_loss'].append(tg_loss)\n",
    "    history['tdisc_loss'].append(td_loss)\n",
    "    history['tl1_loss'].append(tl1_loss)\n",
    "        \n",
    "    # validation loop\n",
    "    g_losses = []\n",
    "    d_losses = []\n",
    "    l1_losses = []\n",
    "    for loop in range(val_step_cnt):\n",
    "\n",
    "        print(f\"\\rV {epoch + 1:4d} {loop+1:3d}\", end=\"\")\n",
    "\n",
    "        try:\n",
    "            x, y = next(vit)\n",
    "        except Exception as e:\n",
    "            vit = iter(vdgen)\n",
    "            x, y = next(vit)\n",
    "\n",
    "        vg_loss, vl1_loss, vd_loss = val_step(x, y)\n",
    "\n",
    "        g_losses.append(vg_loss.numpy())\n",
    "        d_losses.append(vd_loss.numpy())\n",
    "        l1_losses.append(vl1_loss.numpy())\n",
    "\n",
    "    vg_loss = np.average(np.array(g_losses))\n",
    "    vd_loss = np.average(np.array(d_losses))\n",
    "    vl1_loss = np.average(np.array(l1_losses))\n",
    "\n",
    "    history['vgen_loss'].append(vg_loss)\n",
    "    history['vdisc_loss'].append(vd_loss)\n",
    "    history['vl1_loss'].append(vl1_loss)\n",
    "    \n",
    "    if save_load_interval > 0 and epoch > 0 and epoch % save_load_interval == 0:\n",
    "        save_all(prefix=save_load_prefix)\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        img_save_path = os.path.join(cfg.MODEL_SAVE_PATH, save_load_prefix, \"imgs\")\n",
    "        if not os.path.exists(img_save_path):\n",
    "            os.mkdir(img_save_path)\n",
    "\n",
    "        img_save_file = os.path.join(img_save_path, f'result_{img_idx:04d}.png')\n",
    "        while os.path.exists(img_save_file):\n",
    "            img_idx += 1        \n",
    "            img_save_file = os.path.join(img_save_path, f'result_{img_idx:04d}.png')\n",
    "\n",
    "        show_predicts(save_path=img_save_file)\n",
    "\n",
    "    end = time()\n",
    "    elapsed_time = int(end-start)\n",
    "\n",
    "    print(f\"\\r{epoch + 1:4d} {elapsed_time: 3d}s \",\n",
    "            f\"train g-loss:{tg_loss:.2f}  L1:{tl1_loss:.2f}  d-loss:{td_loss:.2f} \", \n",
    "            f\"val g-loss:{vg_loss:.2f}  L1:{vl1_loss:.2f}  d-loss:{vd_loss:.2f}\")\n",
    "\n",
    "batch_end = time()\n",
    "batch_time = int(batch_end-batch_start)\n",
    "        \n",
    "print(f\"train completed ({batch_time//60}m {batch_time%60}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_all(prefix=save_load_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = load_all(prefix=save_load_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predicts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e6debceb626036820d184549e25d059a55b6b8771e25bc8133db281d329c34fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
